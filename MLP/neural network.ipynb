{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cc4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.numpy_pickle_utils import xrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c965848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size=3\n",
    "hidden_layer_size=4\n",
    "number_hidden_layers=1\n",
    "output_layer_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c285314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #Sigmoid Activation Function \n",
    "\treturn 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x): #Derivative of Sigmoid function\n",
    "\treturn x*(1-x)\n",
    "\n",
    "def arctan(x): #Hyperbolic Tangent Activation Function \n",
    "\treturn numpy.tanh(x)   \n",
    "    \n",
    "def arctan_deriv(x): #Derivative of Hyperbolic Tangent function\n",
    "\treturn 1-x**2      \n",
    "\n",
    "def ReLU(x): #ReLU Activation Function \n",
    "\treturn max(0.0, x)   \n",
    "\n",
    "def ReLU_deriv(x): #Derivative of ReLU function\n",
    "\treturn 1*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c5c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]]) #Half Dataset for Training\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96984dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(92) #This is a good performing random seed. In general, make sure to fix your random seed in some fashion to ensure code works properly first! \n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "weights_0 = 2*np.random.random((input_layer_size,hidden_layer_size)) - 1\n",
    "weights_1 = 2*np.random.random((hidden_layer_size,output_layer_size)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ee239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.4989274862080606\n",
      "Error:0.0005404889275701136\n",
      "Error:0.00021721906010979761\n",
      "Error:0.0001303239755950038\n",
      "Error:9.145419026283046e-05\n",
      "Error:6.97794033565973e-05\n",
      "Error:5.6083696997266074e-05\n",
      "Error:4.6700949534548086e-05\n",
      "Error:3.989816268325934e-05\n",
      "Error:3.475464819413776e-05\n",
      "Error:3.0738010645921606e-05\n",
      "Error:2.751988455571169e-05\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# TRAINING #\n",
    "#############################\n",
    "for j in xrange(60000): #60,000 training iterations \n",
    "\n",
    "\t# Feed forward through neural network\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(layer_0,weights_0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1))\n",
    "\n",
    "    # How much did we miss the target value?\n",
    "    layer_2_true_error = 0.5*np.sum(np.power((y-layer_2),2)) #L2 norm error\n",
    "    layer_2_error = y - layer_2 #Partial derivative of L2 norm error (for backpropagation)\n",
    "    \n",
    "    if (j% 5000) == 0:\n",
    "        print (\"Error:\" + str(layer_2_true_error)) #Print error every 5000 iterations\n",
    "    # In what direction is the target value? \n",
    "    layer_2_delta = layer_2_error*sigmoid_deriv(layer_2)\n",
    "\n",
    "    # How much did each layer_1 value contribute to the layer_2 error?\n",
    "    layer_1_error = layer_2_delta.dot(weights_1.T)\n",
    "    \n",
    "    # In what direction is the target layer_1?\n",
    "    layer_1_delta = layer_1_error * sigmoid_deriv(layer_1)\n",
    "    \n",
    "    # Update the weights in the neural network in the direction of gradient descent\n",
    "    weights_1 += layer_1.T.dot(layer_2_delta)\n",
    "    weights_0 += layer_0.T.dot(layer_1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42045447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Full Dataset After Training:\n",
      "[[0.32379775]\n",
      " [0.9269275 ]\n",
      " [0.92116144]\n",
      " [0.00229311]\n",
      " [0.0018005 ]\n",
      " [0.996365  ]\n",
      " [0.99632265]\n",
      " [0.00444932]]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# TESTING #\n",
    "#############################\n",
    "X_new=np.array([[0,0,0],\n",
    "            [0,1,0],\n",
    "            [1,0,0],\n",
    "            [1,1,0],[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]]) #Full dataset for testing, normally is a different set of data than used for training\n",
    "#Feedforward using the new weights\n",
    "layer_0 = X_new\n",
    "layer_1 = sigmoid(np.dot(layer_0,weights_0))\n",
    "layer_2 = sigmoid(np.dot(layer_1,weights_1))\n",
    "print (\"Output of Full Dataset After Training:\")\n",
    "print (layer_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
